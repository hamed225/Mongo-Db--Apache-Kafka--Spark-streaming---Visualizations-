{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQddsCMo2Nf2"
   },
   "source": [
    "# Stream Processing using Apache Spark Streaming\n",
    "## (a) Streaming application\n",
    "### Importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1h-Camf2J1t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "import pygeohash as gh     # to calculat geo-hash value\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkVkAxJ34jYB"
   },
   "source": [
    "### Merging data\n",
    "* Merge data coming from three different stream based on following conditions:\n",
    "  * Join the streams based on the location (i.e. latitude and longitude)\n",
    "and create the data model developed in Task A.\n",
    "  * Find if two locations are close to each other or not. You can do this\n",
    "by implementing the geo-hashing algorithm or find the library that\n",
    "does the job for you. Use precision 5. The precision determines the\n",
    "number of characters in the Geohash.\n",
    "  * If we receive the data from two different satellites AQUA and\n",
    "TERRA for the same location, then average the ‘surface\n",
    "temperature’ and ‘confidence’.\n",
    "  * If the streaming application has the data from only one producer\n",
    "(Producer 1), it implies that there was no fire at that time and we can\n",
    "store the climate data into MongoDB straight away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKf1_u2c2J10"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function is used to merge the data coming from three different Kafka producers\n",
    "'''\n",
    "def merge_data(batch,db):\n",
    "    climate_list = []\n",
    "    climate_coll = db['ch']\n",
    "    hotspot_coll = db['hh']\n",
    "    aqua = None\n",
    "    terra = None\n",
    "    for d in batch:\n",
    "        d_copy = json.loads(d[1]).copy()\n",
    "        points = d_copy['location']['coordinates']\n",
    "        \n",
    "        # gets geo-hash string value with precision 5\n",
    "        geo_hash =  gh.encode(points[0],points[1],precision = 5)\n",
    "        d_copy['geo-hash'] = geo_hash\n",
    "        \n",
    "        # identify the data source and segregate based on that\n",
    "        if d[0]=='climate':\n",
    "            climate_list.append(d_copy)\n",
    "        \n",
    "        elif d[0]=='AQUA':\n",
    "            aqua = d_copy.copy()\n",
    "\n",
    "        else:\n",
    "            terra = d_copy.copy()\n",
    "    \n",
    "    # if there is no data stream from source aqua and terra\n",
    "    if not aqua and not terra:\n",
    "        if climate_list:\n",
    "            climate_coll.insert_many(climate_list, ordered = False)\n",
    "            print(\"Inserted climate list1 {}\\n\\n\".format(climate_list))\n",
    "        return\n",
    "    \n",
    "    sat_dict = None\n",
    "    \n",
    "    # if data stream is available for both aqua and terra\n",
    "    if aqua and terra:\n",
    "      \n",
    "        # if their geo-hash value matches, merge the data\n",
    "        if aqua['geo-hash'] == terra['geo-hash']:\n",
    "            sat_dict = {}\n",
    "            sat_dict['confidence'] = (aqua['confidence'] + terra['confidence'])/2\n",
    "            sat_dict['surface_temperature'] = (aqua['surface_temperature'] + terra['surface_temperature'])/2\n",
    "            sat_dict['location'] = {\"coordinates\": [gh.decode(aqua['geo-hash'])[0], gh.decode(aqua['geo-hash'])[1]], \"type\": \"Point\"}\n",
    "            merge_id = hotspot_coll.insert_one(sat_dict).inserted_id\n",
    "            print(\"Document inserted common {}\\n\\n\".format(merge_id))\n",
    "    \n",
    "    # if only aqua data present\n",
    "    elif aqua:\n",
    "        del aqua['geo-hash']\n",
    "        aqua_id = hotspot_coll.insert_one(aqua).inserted_id\n",
    "        print(\"Document inserted aqua {}\\n\\n\".format(aqua_id))\n",
    "    # if only terra data is present\n",
    "    else:\n",
    "        del terra['geo-hash']\n",
    "        terra_id = hotspot_coll.insert_one(terra).inserted_id\n",
    "        print(\"Document inserted terra {}\\n\\n\".format(terra_id))\n",
    "    \n",
    "    # merging with climate streaming data.\n",
    "    if sat_dict:\n",
    "        hash_value = sat_dict['geo-hash']\n",
    "        for c in climate_list:\n",
    "            if c['geo-hash'] == hash_value:\n",
    "                c['surface_temperaure'] = sat_dict['surface_temperature']\n",
    "                c['confidence'] = sat_dict['confidence']\n",
    "                c['hotspot_data'] = [merge_id]\n",
    "    else:\n",
    "        for c in climate_list:\n",
    "            if c['geo-hash'] == aqua['geo-hash']:\n",
    "                c['surface_temperaure'] = aqua['surface_temperature']\n",
    "                c['confidence'] = aqua['confidence']\n",
    "                c['hotspot_data'] = [aqua_id]\n",
    "            \n",
    "            elif c['geo-hash'] == terra['geo-hash']:\n",
    "                c['surface_temperaure'] = terra['surface_temperature']\n",
    "                c['confidence'] = terra['confidence']\n",
    "                c['hotspot_data'] = [terra_id]\n",
    "    if climate_list:\n",
    "        climate_coll.insert_many(climate_list, ordered = False)\n",
    "        print(\"Inserted climate list2 {}\\n\\n\".format(climate_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qs4MM4xE2J15",
    "outputId": "2c476172-b01b-4c34-b900-5fbb604b1a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This function is used to consume data from kafka producer and process using\n",
    "Apache Spark data stream processing and then finally inserting it into mongoDB\n",
    "'''\n",
    "def sendDataToDB(iter_):\n",
    "    client = MongoClient()\n",
    "    db = client['fit5148_assignment_db']\n",
    "    try:\n",
    "        merge_data(iter_,db)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "            \n",
    "    client.close()\n",
    "\n",
    "# setting batch interval to 10 seconds\n",
    "batch_interval = 10\n",
    "topic = \"climate\"\n",
    "\n",
    "# setting local streaming context with two execution threads\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    \n",
    "\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "topic_list = [topic,\"AQUA\",\"TERRA\"]\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc,topic_list , {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week11-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "print(lines)\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YmpfE6j2J2D"
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFr0cOvA2J2N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_TaskC_Streaming_Application_v3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
